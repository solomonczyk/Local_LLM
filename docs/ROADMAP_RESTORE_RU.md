# Roadmap: Local Consilium — от текущего состояния до “полной версии”

## Реальность и сроки (личный ориентир)
Короткая оценка: до полностью работоспособного продукта для себя — примерно 1–2 недели.

Почему так быстро:
- решение governance/observability уже готово;
- explainability, rollback, approval, learning loop, CI‑дисциплина и baseline — закрыты.

Цель для себя: система помогает принимать технические решения лучше, спокойнее и быстрее, чем в одиночку.

Минимум, который остался:
- единый entrypoint (CLI/HTTP/Make);
- один удобный интерфейс просмотра (например, отчёт/лог).

Оценка по дням:
- 1–2 дня — entrypoint;
- 2–3 дня — удобный вывод/нотификации;
- 2–3 дня — шлифовка под реальные кейсы.

## 0) Цель (что считается “готово”)
Полная версия системы — это локальный ассистент для кодинга уровня “Cursor/Codex‑класс” по следующим критериям:
- **Кодинг на разных языках**: уверенная работа минимум с Python, JS/TS, Go, SQL (+ расширяемо).
- **Tool‑use как у Cursor**: поиск/чтение/запись/редактирование/удаление файлов, git, запуск тестов/линтера/команд, работа с БД — **только через подтверждение в UI**.
- **Consilium‑режим**: несколько специализированных агентов обсуждают задачу и дают единый план/решение; Director (GPT‑5.2) — опциональная финальная экспертиза.
- **Local‑first + минимальный облак**о: по умолчанию всё локально; облако только по тумблеру/режиму и с санитизацией.
- **Масштабируемость**: есть понятный путь расширения (новые агенты, новые инструменты, более крупные модели, дообучение).
- **Контроль качества**: есть eval‑набор и регресс‑гейты, чтобы качество росло измеримо.

## 1) Текущее состояние (снимок)
- Docker‑стек поднят: Postgres + LLM backend (сейчас `llama.cpp + GGUF`) + `agent-system` + nginx.
- Есть LoRA‑артефакты и локальная HF‑база (см. `docs/project_log_until_now.md`).
- Ключевой разрыв: **LoRA не подключена в runtime**, а `serve_lora.py` (transformers+peft) требует доводки, если выбираем этот путь.
- Инструменты уже существуют и должны оставаться “approval‑gated” (UI‑кнопка).

## 2) Архитектурные принципы (фиксируем как правило проекта)
- **Сначала выводы → потом детали** (все агенты).
- **Не переписывать код без запроса**: только точечные изменения под явную цель.
- **Детерминизм > токены**: где можно — используем `rg`, git, тесты, статический анализ, а не рассуждения LLM.
- **Два контура inference** (рекомендовано):
  - **Training/Eval**: HF base + LoRA (transformers/peft) — источник “истины” по обучению.
  - **Runtime**: GGUF/llama.cpp или Ollama — быстрый ежедневный режим; LoRA встраивается через экспорт (merge+quant) или отдельный backend.
- **RAG/KB отдельно от SFT/LoRA** (не смешиваем пайплайны).
- **Секреты не в git**: ключи/пароли только в `.env`, проверка на утечки перед push.

---

## Этап 0 — Определяем “как меряем успех” (обязательный гейт)
**Цель:** чтобы любые следующие изменения были измеримы и не превращались в “похоже стало лучше”.

### Что делаем
- Зафиксировать список поддерживаемых языков (MVP: Python, JS/TS, Go, SQL).
- Зафиксировать минимальный набор инструментов “уровня Cursor” (см. Этап 2).
- Определить режимы: FAST/STANDARD/CRITICAL и правила вызова Director.
- Описать политику приватности (что можно/нельзя отправлять в облако).
- Собрать **eval‑набор “боевых задач”** (20–50 задач), который отражает твою реальную работу.

### Артефакты
- `consilium.md` — роли/режимы/политика.
- Документ “Eval Suite v1” (список задач + ожидаемые результаты).

### Exit‑criteria (нельзя идти дальше, пока не выполнено)
- Есть фиксированный eval‑набор v1 и понятные метрики “pass/fail”.
- Принято решение по runtime‑стратегии:
  - A) остаёмся на GGUF/Ollama для ежедневного режима + экспорт из LoRA, или
  - B) переходим на transformers+peft как основной runtime.

---

## Этап 1 — Стабильный рабочий MVP (включается и полезен каждый день)
**Цель:** система поднимается одной командой, UI работает, local LLM отвечает без моков, инструменты доступны и подтверждаются кнопкой.

### Что делаем
- Привести Docker‑compose к состоянию “поднялось и работает”:
  - единые порты/healthchecks,
  - понятные `env`‑переменные,
  - отсутствие неявных fallback’ов в mock.
- Привести nginx к простому маршруту UI/LLM/Tools (без лишних конфигов).
- Закрыть “секретный долг”: убедиться, что ключи/приватные ключи/токены не лежат в git и не попадают в логи.

### Тестирование (минимально необходимое)
- `docker compose up -d --build` (core профили).
- Проверка `/health` и `/v1/models` у LLM, `/health` tools, UI открывается.
- Smoke‑задача: поиск по репо + чтение файла + создание нового файла через tool‑approval.

### Exit‑criteria
- UI доступен через nginx и напрямую.
- LLM отвечает “по делу” (не шаблон/мок) и возвращает модели в `/v1/models`.
- Любое destructive действие попадает в Pending Actions и требует подтверждения.

---

## Этап 2 — Инструменты уровня Cursor (ядро продуктивности)
**Цель:** закрыть реальный цикл разработки “в одной системе” без ручного переключения.

### Must‑have инструменты (MVP‑набор)
- **Repo/Context**: дерево проекта, быстрый `rg`‑поиск, чтение фрагментов файлов, сбор “контекст‑пакета” по задаче.
- **Файлы**: create/read/edit/move/copy/delete с preview/diff, ограничение workspace‑root.
- **Git**: status/diff/log/branch + безопасные операции; опасные (push/reset) — отдельный уровень и явное подтверждение.
- **Команды**: запуск тестов/линтера/сборки с allowlist, таймаутами и логами.
- **DB (опционально)**: подключение Postgres для памяти/проектных данных, безопасные запросы.

### UX требования
- В UI видно: что будет сделано, какие файлы затронуты, какой diff, какие риски.
- Одна кнопка “Approve”, одна — “Clear”.
- “Dry‑run” всегда перед destructive действием.

### Тестирование
- Набор unit‑тестов для tool‑валидации.
- Набор e2e “tool‑flows” (создать файл → изменить → запустить тест → откатить).

### Exit‑criteria
- 90% задач из eval‑набора (где нужны инструменты) выполняются без ручных правок файлов.
- Нет “молчаливых” изменений вне approval‑потока.

---

## Этап 3 — Consilium v1 (многоагентный протокол, который реально помогает)
**Цель:** консилиум даёт более качественные решения, чем один агент, но не “съедает” контекст/токены.

### Что делаем
- Ввести единый протокол ответов агентов: **выводы → план → риски → нужные инструменты**.
- Smart routing: подключать агентов по триггерам (security/devops/qa/ux).
- “Сжатие контекста”: агенты получают разные срезы (дифф/список файлов/резюме), а не весь репозиторий.
- Director‑гейтинг: Director включается только в CRITICAL или по кнопке; в облако уходит только санитизированный summary.

### Тестирование
- Прогон eval‑набора с логированием:
  - качество решения,
  - число tool‑вызовов,
  - число обращений к Director.

### Exit‑criteria
- Consilium стабильно улучшает качество на eval‑наборе (рост pass‑rate/снижение итераций).
- Director‑вызовы редкие и оправданные (нет “слива контекста по умолчанию”).

---

## Этап 4 — Память/KB + RAG (локальное знание вместо токенов)
**Цель:** система “помнит” проект и твои правила без постоянной пересылки контекста в промпт.

### Что делаем
- KB по ролям (security checklist, arch rules, testing strategy).
- Индексация репо (минимум: поиск/теги/кэш резюме; максимум: символ‑индекс по языкам).
- Локальные embeddings (опционально) + RAG‑слой для выбора релевантных фрагментов.
- Версионирование KB и “правил проекта”.

### Exit‑criteria
- Сокращается средний размер контекста (токены) при сохранении/росте качества.
- Новая задача быстрее получает релевантные файлы/правила без ручного выбора.

---

## Этап 5 — Eval/регрессия как обязательный гейт (качество не деградирует)
**Цель:** любое улучшение измеримо; любое ухудшение ловится автоматически.

### Что делаем
- Ввести “golden” прогон eval‑набора (локально, без сети).
- Метрики: pass‑rate, среднее число итераций, число tool‑ошибок, latency, обращения к Director.
- “Release gate”: нельзя обновлять модель/промпты/инструменты без зелёного прогона.

### Exit‑criteria
- Есть автоматический прогон eval и понятный отчёт.

---

## Этап 6 — Дообучение v1 (LoRA) + экспорт в runtime
**Цель:** получить заметный прирост в “вайбкодинге” и tool‑use на твоих задачах.

### Что делаем (пайплайн)
- Датасет:
  - старт: CodeSearchNet + твои локальные примеры (без секретов),
  - расширение: multi‑language (TS/Go/SQL) и “инструментальные” примеры (план → tool calls → diff).
- Обучение:
  - LoRA‑конфиг фиксируется в repo (без весов),
  - веса хранятся локально, с checksum/версией.
- Оценка:
  - прогон eval‑набора до/после,
  - фиксируем прирост и регрессии.
- Деплой:
  - если runtime = GGUF/Ollama: merge LoRA → экспорт/квантизация → выкладка как артефакт,
  - если runtime = transformers+peft: подключение адаптера напрямую.

### Exit‑criteria
- Улучшение на eval‑наборе подтверждено (не “кажется лучше”).
- Есть воспроизводимый процесс: train → eval → export → run.

---

## Этап 7 — Масштабирование качества (к “уровню Opus для кодинга”)
**Цель:** поднять планку качества без лавинообразного роста затрат и без слива данных.

### Направления
- Multi‑model routing: маленькая локальная модель для FAST, более сильная (локальная/удалённая) — только для CRITICAL.
- Углубление tool‑use:
  - обучение на трассах успешных задач,
  - строгий формат tool‑вызовов,
  - отказоустойчивость (repair‑loop, ретраи, fallback).
- Расширение “пространств”:
  - SSH/VPS (строго opt‑in),
  - GitHub (строго opt‑in, минимальные права),
  - изоляция контекстов и ключей.

### Exit‑criteria
- Качество и стабильность растут, а число облачных вызовов остаётся низким и контролируемым.
