# Локальная LLM-система — консолидированные технические заметки

## Резюме
Документ объединяет материалы из Local_LLM в единый технический источник. Система — локальный мультиагентный «vibe‑coding» ассистент с Консилиумом (агенты) и Директором (GPT‑5.2), который выдаёт итоговый экспертный вывод.

## Цель
Создать локальную систему, повышающую продуктивность в кодинге:
- Несколько агентов обсуждают задачу и формируют лучший общий ответ.
- Директор (GPT‑5.2) оценивает вывод Консилиума и выдаёт экспертное решение.
- Инструменты уровня Codex/Cursor/Kiro с обязательным подтверждением.
- Локальная LLM — основной движок; облако только для Директора.

## Архитектура (целевая)
- UI (Gradio) -> Оркестратор -> Консилиум (multi‑agent) -> Директор (GPT‑5.2).
- Tool‑сервер для CRUD, поиска, shell, git, БД, системной информации (с подтверждением).
- RAG/KB по агентам с общими правилами качества и версионирования.

## Роли агентов
Ожидаемые роли:
- dev: реализация и практические шаги
- security: риски и безопасность
- qa: стратегия тестирования
- architect: структура и масштабируемость
- ux: пользовательский опыт и интерфейс
- seo: обнаруживаемость и контент‑стратегия
- director: финальный вывод (GPT‑5.2)

## Стратегия LLM
- Базовая локальная модель: Qwen2.5‑Coder‑1.5B
- Дообучение: LoRA (веса проекта)
- Причина выбора: 1.5B оптимальна для latency, скорости итераций и мультиагентной нагрузки.
- Путь апгрейда: 7–8B после стабилизации пайплайна и eval.

## Пайплайны данных
Нужны два независимых пайплайна:
1) RAG/KB (быстрое и дешёвое улучшение качества)
2) SFT/LoRA (только после выявления устойчивых паттернов)
Смешивать эти потоки нельзя — это снижает устойчивость и качество.

## База знаний (KB)
KB разделяется по доменам агентов:
- security_checklist.md
- architecture_review.md
- testing_strategy.md
У каждого агента своя база; общие правила по метаданным и версии.

## LoRA‑обучение
Скрипт обучения:
- train_lora.py
- базовая модель: Qwen/Qwen2.5‑Coder‑1.5B
- output: lora_qwen2_5_coder_1_5b_python

Артефакты LoRA:
- adapter_model.safetensors (веса проекта)
- adapter_config.json

Базовые веса:
- лежат в HuggingFace cache (шардированные model.safetensors)
- в Windows возможен режим без symlink (предупреждение HF cache)

## Инфраструктура и деплой
Наблюдаемая архитектура:
- Docker сервисы: UI, LLM API, Tools API, Postgres
- Reverse proxy через nginx
- Ранее использовался ngrok для публикации локального LLM
- Конфликт портов 80/443 решается общим edge‑proxy

## Известные проблемы (из заметок)
- LLM сервер может работать в mock‑режиме без реальной модели.
- Контейнеры стартуют, но сервисы не слушают порты (устаревший образ или ошибки в коде).
- Конфликт портов 80/443 с другими проектами.
- Smart‑routing плохо ловит русские триггеры.

## Риски
- Без LoRA‑весов система деградирует до шаблонных ответов.
- Хранение весов в git неустойчиво (размер/лицензии/воспроизводимость).
- ngrok добавляет внешнюю зависимость и риск отказа.

## Обязательные артефакты (вне репозитория)
- LoRA веса для Qwen2.5‑Coder‑1.5B:
  - adapter_model.safetensors
  - adapter_config.json
- Базовая модель в HF cache (model.safetensors shards)

## Следующие шаги (операционные)
1) Найти и восстановить LoRA‑веса.
2) Переключить LLM сервер на реальную инференс‑модель (serve_lora.py или GGUF).
3) Перезапустить аудит Consilium на реальных задачах.
4) Расширить smart‑routing триггеры под русский язык.
5) Документировать bootstrap модели и окружения.

## Источники
Сведения собраны из документов Local_LLM:
- dok1.docx: датасеты для code‑LLM
- dok2.docx: лог LoRA‑обучения и HF cache
- dok3.docx: настройка KB по агентам
- dok4.docx: разделение RAG vs SFT/LoRA
- dok5.docx: аргументы выбора 1.5B
- dok6.docx: гибридная архитектура и проблемы деплоя
- dok7.docx: edge proxy и стратегия портов
- dok8.docx: правки docker‑compose
- dok9.docx: диагностика build/image
