#!/usr/bin/env python3
"""
–¢–µ—Å—Ç –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–∞–±–æ—Ç—ã –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã
–û—Ü–µ–Ω–∏–≤–∞–µ—Ç: –ø–æ–ª–Ω–æ—Ç—É, —Ç–æ—á–Ω–æ—Å—Ç—å, —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å, –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å –æ—Ç–≤–µ—Ç–æ–≤
"""
import json
import time
from typing import Dict, List, Any
from agent_runtime.orchestrator import get_orchestrator


class MultiAgentQualityTester:
    """–¢–µ—Å—Ç–µ—Ä –∫–∞—á–µ—Å—Ç–≤–∞ –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã"""

    def __init__(self):
        self.orchestrator = get_orchestrator()
        self.test_results = []

    def evaluate_response_quality(
        self, task: str, response: Dict[str, Any], expected_elements: List[str] = None
    ) -> Dict[str, Any]:
        """–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –æ—Ç–≤–µ—Ç–∞"""

        if not response.get("success", False):
            return {
                "overall_score": 0,
                "completeness": 0,
                "accuracy": 0,
                "relevance": 0,
                "consistency": 0,
                "error": response.get("error", "Unknown error"),
            }

        # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞
        if response.get("mode") == "consilium":
            # –ö–æ–Ω—Å–∏–ª–∏—É–º - –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –º–Ω–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤
            opinions = response.get("opinions", {})
            response_text = "\n".join([f"{agent}: {data.get('opinion', '')}" for agent, data in opinions.items()])
            director_decision = response.get("director_decision", "")
            if director_decision:
                response_text += f"\nDirector: {director_decision}"
        else:
            # –û–¥–∏–Ω–æ—á–Ω—ã–π –∞–≥–µ–Ω—Ç
            response_text = response.get("response", "")

        # –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
        completeness = self._evaluate_completeness(task, response_text, expected_elements)
        accuracy = self._evaluate_accuracy(task, response_text)
        relevance = self._evaluate_relevance(task, response_text)
        consistency = self._evaluate_consistency(response)

        overall_score = (completeness + accuracy + relevance + consistency) / 4

        return {
            "overall_score": round(overall_score, 2),
            "completeness": round(completeness, 2),
            "accuracy": round(accuracy, 2),
            "relevance": round(relevance, 2),
            "consistency": round(consistency, 2),
            "response_length": len(response_text),
            "response_text": response_text[:200] + "..." if len(response_text) > 200 else response_text,
        }

    def _evaluate_completeness(self, task: str, response: str, expected_elements: List[str] = None) -> float:
        """–û—Ü–µ–Ω–∫–∞ –ø–æ–ª–Ω–æ—Ç—ã –æ—Ç–≤–µ—Ç–∞ (0-10)"""
        if not response or len(response) < 50:
            return 2.0  # –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π –æ—Ç–≤–µ—Ç

        score = 5.0  # –ë–∞–∑–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –æ–∂–∏–¥–∞–µ–º—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤
        if expected_elements:
            found_elements = sum(1 for elem in expected_elements if elem.lower() in response.lower())
            element_score = (found_elements / len(expected_elements)) * 3.0
            score += element_score

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –æ—Ç–≤–µ—Ç–∞
        if any(marker in response for marker in ["1.", "2.", "3.", "‚Ä¢", "-", "**", "##"]):
            score += 1.0  # –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –ø—Ä–∏–º–µ—Ä–æ–≤ –∏–ª–∏ –¥–µ—Ç–∞–ª–µ–π
        if any(word in response.lower() for word in ["–Ω–∞–ø—Ä–∏–º–µ—Ä", "example", "–ø—Ä–∏–º–µ—Ä", "–¥–µ—Ç–∞–ª–∏"]):
            score += 1.0

        return min(score, 10.0)

    def _evaluate_accuracy(self, task: str, response: str) -> float:
        """–û—Ü–µ–Ω–∫–∞ —Ç–æ—á–Ω–æ—Å—Ç–∏ –æ—Ç–≤–µ—Ç–∞ (0-10)"""
        score = 7.0  # –ë–∞–∑–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞ (–ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –æ—à–∏–±–æ–∫ –∏–ª–∏ –Ω–µ—Ç–æ—á–Ω–æ—Å—Ç–µ–π
        error_indicators = ["[error]", "[llm_error]", "[connection_error]", "–Ω–µ –º–æ–≥—É", "–Ω–µ –∑–Ω–∞—é", "–æ—à–∏–±–∫–∞", "error"]

        for indicator in error_indicators:
            if indicator.lower() in response.lower():
                score -= 2.0
                break

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫—É—é –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –∑–∞–¥–∞—á
        if "security" in task.lower():
            security_terms = ["vulnerability", "authentication", "authorization", "encryption"]
            if any(term in response.lower() for term in security_terms):
                score += 1.0

        if "architecture" in task.lower():
            arch_terms = ["scalability", "design", "pattern", "component"]
            if any(term in response.lower() for term in arch_terms):
                score += 1.0

        return max(min(score, 10.0), 0.0)

    def _evaluate_relevance(self, task: str, response: str) -> float:
        """–û—Ü–µ–Ω–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –æ—Ç–≤–µ—Ç–∞ (0-10)"""
        if not response:
            return 0.0

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –∑–∞–¥–∞—á–∏
        task_words = set(task.lower().split())
        response_words = set(response.lower().split())

        # –ü–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        common_words = task_words.intersection(response_words)
        relevance_ratio = len(common_words) / len(task_words) if task_words else 0

        base_score = relevance_ratio * 6.0

        # –ë–æ–Ω—É—Å—ã –∑–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—É—é —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å
        if len(response) > 100:  # –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø–æ–¥—Ä–æ–±–Ω—ã–π –æ—Ç–≤–µ—Ç
            base_score += 2.0

        if any(word in response.lower() for word in ["—Ä–µ–∫–æ–º–µ–Ω–¥—É—é", "–ø—Ä–µ–¥–ª–∞–≥–∞—é", "—Å–ª–µ–¥—É–µ—Ç", "recommend"]):
            base_score += 1.0  # –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏

        if any(word in response.lower() for word in ["–ø–æ—Ç–æ–º—É —á—Ç–æ", "—Ç–∞–∫ –∫–∞–∫", "because", "since"]):
            base_score += 1.0  # –û–±—ä—è—Å–Ω–µ–Ω–∏—è

        return min(base_score, 10.0)

    def _evaluate_consistency(self, response: Dict[str, Any]) -> float:
        """–û—Ü–µ–Ω–∫–∞ –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏ –æ—Ç–≤–µ—Ç–∞ (0-10)"""
        score = 8.0  # –ë–∞–∑–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞

        # –î–ª—è –∫–æ–Ω—Å–∏–ª–∏—É–º–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –º–Ω–µ–Ω–∏–π
        if response.get("mode") == "consilium":
            opinions = response.get("opinions", {})
            if len(opinions) > 1:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–π
                opinion_texts = [data.get("opinion", "") for data in opinions.values()]

                # –ü—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—è
                positive_indicators = ["—Ö–æ—Ä–æ—à–æ", "–æ—Ç–ª–∏—á–Ω–æ", "—Ä–µ–∫–æ–º–µ–Ω–¥—É—é", "good", "excellent"]
                negative_indicators = ["–ø–ª–æ—Ö–æ", "–Ω–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É—é", "–ø—Ä–æ–±–ª–µ–º–∞", "bad", "issue"]

                positive_count = sum(
                    1 for text in opinion_texts for indicator in positive_indicators if indicator in text.lower()
                )
                negative_count = sum(
                    1 for text in opinion_texts for indicator in negative_indicators if indicator in text.lower()
                )

                if positive_count > 0 and negative_count > 0:
                    score -= 1.0  # –ï—Å—Ç—å –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—è, –Ω–æ —ç—Ç–æ –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–æ—Ä–º–∞–ª—å–Ω–æ

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—É—é –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å
        if response.get("success") and not response.get("response", "").strip():
            score -= 3.0  # –£—Å–ø–µ—Ö, –Ω–æ –ø—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç

        return max(score, 0.0)

    def run_quality_tests(self) -> Dict[str, Any]:
        """–ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤ –∫–∞—á–µ—Å—Ç–≤–∞"""

        test_cases = [
            {
                "name": "Simple Question",
                "task": "What is Python?",
                "mode": "single",
                "expected_elements": ["programming", "language", "high-level"],
                "expected_score_range": (6.0, 9.0),
            },
            {
                "name": "Security Analysis",
                "task": "Review JWT authentication security",
                "mode": "consilium",
                "expected_elements": ["security", "token", "authentication", "vulnerability"],
                "expected_score_range": (7.0, 10.0),
            },
            {
                "name": "Architecture Design",
                "task": "Design microservice architecture for e-commerce",
                "mode": "consilium",
                "expected_elements": ["microservice", "architecture", "scalability", "design"],
                "expected_score_range": (7.0, 10.0),
            },
            {
                "name": "Code Review",
                "task": "Review this code for best practices and potential issues",
                "mode": "single",
                "expected_elements": ["code", "review", "best practices", "issues"],
                "expected_score_range": (6.0, 9.0),
            },
            {
                "name": "Two-pass Triage",
                "task": "Create a simple hello world function",
                "mode": "twopass",
                "expected_elements": ["function", "hello", "world"],
                "expected_score_range": (6.0, 9.0),
            },
        ]

        results = []

        print("üß™ –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤ –∫–∞—á–µ—Å—Ç–≤–∞ –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã")
        print("=" * 60)

        for i, test_case in enumerate(test_cases, 1):
            print(f"\nüìã –¢–µ—Å—Ç {i}/{len(test_cases)}: {test_case['name']}")
            print(f"   –ó–∞–¥–∞—á–∞: {test_case['task'][:50]}...")
            print(f"   –†–µ–∂–∏–º: {test_case['mode']}")

            start_time = time.time()

            # –í—ã–ø–æ–ª–Ω—è–µ–º —Ç–µ—Å—Ç
            if test_case["mode"] == "consilium":
                response = self.orchestrator.execute_task(test_case["task"], use_consilium=True)
            elif test_case["mode"] == "twopass":
                response = self.orchestrator.execute_task(test_case["task"], two_pass=True)
            else:
                response = self.orchestrator.execute_task(test_case["task"])

            execution_time = time.time() - start_time

            # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
            quality = self.evaluate_response_quality(test_case["task"], response, test_case["expected_elements"])

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –æ–∂–∏–¥–∞–µ–º–æ–º—É –¥–∏–∞–ø–∞–∑–æ–Ω—É
            expected_min, expected_max = test_case["expected_score_range"]
            score_in_range = expected_min <= quality["overall_score"] <= expected_max

            result = {
                "test_name": test_case["name"],
                "task": test_case["task"],
                "mode": test_case["mode"],
                "execution_time": round(execution_time, 2),
                "quality_metrics": quality,
                "expected_range": test_case["expected_score_range"],
                "score_in_range": score_in_range,
                "success": response.get("success", False),
            }

            results.append(result)

            # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            print(f"   ‚è±Ô∏è  –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {execution_time:.2f}—Å")
            print(f"   üìä –û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞: {quality['overall_score']}/10")
            print(f"   üìã –ü–æ–ª–Ω–æ—Ç–∞: {quality['completeness']}/10")
            print(f"   üéØ –¢–æ—á–Ω–æ—Å—Ç—å: {quality['accuracy']}/10")
            print(f"   üîó –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {quality['relevance']}/10")
            print(f"   üîÑ –ö–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å: {quality['consistency']}/10")
            print(f"   ‚úÖ –í –æ–∂–∏–¥–∞–µ–º–æ–º –¥–∏–∞–ø–∞–∑–æ–Ω–µ: {'–î–∞' if score_in_range else '–ù–µ—Ç'}")

            if not response.get("success", False):
                print(f"   ‚ùå –û—à–∏–±–∫–∞: {response.get('error', 'Unknown')}")

        return {"test_results": results, "summary": self._generate_summary(results)}

    def _generate_summary(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–≤–æ–¥–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""

        total_tests = len(results)
        successful_tests = sum(1 for r in results if r["success"])
        tests_in_range = sum(1 for r in results if r["score_in_range"])

        if successful_tests > 0:
            avg_overall = sum(r["quality_metrics"]["overall_score"] for r in results if r["success"]) / successful_tests
            avg_completeness = (
                sum(r["quality_metrics"]["completeness"] for r in results if r["success"]) / successful_tests
            )
            avg_accuracy = sum(r["quality_metrics"]["accuracy"] for r in results if r["success"]) / successful_tests
            avg_relevance = sum(r["quality_metrics"]["relevance"] for r in results if r["success"]) / successful_tests
            avg_consistency = (
                sum(r["quality_metrics"]["consistency"] for r in results if r["success"]) / successful_tests
            )
            avg_execution_time = sum(r["execution_time"] for r in results if r["success"]) / successful_tests
        else:
            avg_overall = avg_completeness = avg_accuracy = avg_relevance = avg_consistency = avg_execution_time = 0

        return {
            "total_tests": total_tests,
            "successful_tests": successful_tests,
            "success_rate": round((successful_tests / total_tests) * 100, 1) if total_tests > 0 else 0,
            "tests_in_expected_range": tests_in_range,
            "range_accuracy": round((tests_in_range / total_tests) * 100, 1) if total_tests > 0 else 0,
            "average_metrics": {
                "overall_score": round(avg_overall, 2),
                "completeness": round(avg_completeness, 2),
                "accuracy": round(avg_accuracy, 2),
                "relevance": round(avg_relevance, 2),
                "consistency": round(avg_consistency, 2),
                "execution_time": round(avg_execution_time, 2),
            },
        }


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""

    tester = MultiAgentQualityTester()
    results = tester.run_quality_tests()

    print("\n" + "=" * 60)
    print("üìä –°–í–û–î–ö–ê –†–ï–ó–£–õ–¨–¢–ê–¢–û–í –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø –ö–ê–ß–ï–°–¢–í–ê")
    print("=" * 60)

    summary = results["summary"]

    print(f"üìã –í—Å–µ–≥–æ —Ç–µ—Å—Ç–æ–≤: {summary['total_tests']}")
    print(f"‚úÖ –£—Å–ø–µ—à–Ω—ã—Ö: {summary['successful_tests']} ({summary['success_rate']}%)")
    print(f"üéØ –í –æ–∂–∏–¥–∞–µ–º–æ–º –¥–∏–∞–ø–∞–∑–æ–Ω–µ: {summary['tests_in_expected_range']} ({summary['range_accuracy']}%)")

    print(f"\nüìä –°—Ä–µ–¥–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞:")
    avg = summary["average_metrics"]
    print(f"   üèÜ –û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞: {avg['overall_score']}/10")
    print(f"   üìã –ü–æ–ª–Ω–æ—Ç–∞: {avg['completeness']}/10")
    print(f"   üéØ –¢–æ—á–Ω–æ—Å—Ç—å: {avg['accuracy']}/10")
    print(f"   üîó –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {avg['relevance']}/10")
    print(f"   üîÑ –ö–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å: {avg['consistency']}/10")
    print(f"   ‚è±Ô∏è  –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è: {avg['execution_time']}—Å")

    # –û—Ü–µ–Ω–∫–∞ –æ–±—â–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ —Å–∏—Å—Ç–µ–º—ã
    overall_quality = avg["overall_score"]
    if overall_quality >= 8.0:
        quality_level = "–û–¢–õ–ò–ß–ù–û"
        emoji = "üèÜ"
    elif overall_quality >= 7.0:
        quality_level = "–•–û–†–û–®–û"
        emoji = "‚úÖ"
    elif overall_quality >= 6.0:
        quality_level = "–£–î–û–í–õ–ï–¢–í–û–†–ò–¢–ï–õ–¨–ù–û"
        emoji = "‚ö†Ô∏è"
    else:
        quality_level = "–¢–†–ï–ë–£–ï–¢ –£–õ–£–ß–®–ï–ù–ò–Ø"
        emoji = "‚ùå"

    print(f"\n{emoji} –û–ë–©–ê–Ø –û–¶–ï–ù–ö–ê –ö–ê–ß–ï–°–¢–í–ê: {quality_level} ({overall_quality}/10)")

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    with open("multiagent_quality_results.json", "w", encoding="utf-8") as f:
        json.dump(results, f, indent=2, ensure_ascii=False)

    print(f"\nüíæ –î–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: multiagent_quality_results.json")

    return overall_quality >= 6.0


if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)
