# Example environment file
# Copy to `.env` and fill real values. Never commit real secrets.

# ===== Security / API keys =====
AGENT_API_KEY=your_secure_api_key_here_minimum_32_characters

# Optional: Director (cloud, only for CRITICAL / low confidence)
OPENAI_API_KEY=
DIRECTOR_MODEL=gpt-5.2

# ===== Database =====
POSTGRES_DB=agent_memory
POSTGRES_USER=agent_user
POSTGRES_PASSWORD=secure_password_here_minimum_16_characters
POSTGRES_HOST=postgres
POSTGRES_PORT=5432

# ===== Workspace (tools are restricted to this root) =====
# In Docker Compose we mount the repo to /workspace
AGENT_WORKSPACE=/workspace

# ===== Local LLM backend =====
# agent-system runs an OpenAI-compatible proxy at :8010 and forwards to this backend:
LLM_SERVER_IMPL=serve_proxy.py
LLM_PROXY_V1=http://llm:8000/v1
AGENT_LLM_URL=http://llm:8000/v1
LLM_BASE=http://llm:8000

# Tools endpoint for agent runtime (inside container)
TOOL_SERVER_URL=http://localhost:8011

# llama.cpp model id (as returned by /v1/models)
AGENT_LLM_MODEL=qwen2.5-coder-1.5b-instruct-q4_k_m.gguf

# ===== Fail-fast controls =====
LLM_STRICT_STARTUP=true
ALLOW_MOCK_LLM=false

# ===== Consilium =====
CONSILIUM_MODE=STANDARD
KB_TOP_K=5
KB_MAX_CHARS=8000
