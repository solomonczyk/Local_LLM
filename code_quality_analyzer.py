#!/usr/bin/env python3
"""
ÐÐ½Ð°Ð»Ð¸Ð·Ð°Ñ‚Ð¾Ñ€ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð° ÐºÐ¾Ð´Ð° Ð¼ÑƒÐ»ÑŒÑ‚Ð¸Ð°Ð³ÐµÐ½Ñ‚Ð½Ð¾Ð¹ ÑÐ¸ÑÑ‚ÐµÐ¼Ñ‹
ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÑ‚: Ð°Ñ€Ñ…Ð¸Ñ‚ÐµÐºÑ‚ÑƒÑ€Ñƒ, ÑÑ‚Ð¸Ð»ÑŒ, ÑÐ»Ð¾Ð¶Ð½Ð¾ÑÑ‚ÑŒ, Ð±ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ð¾ÑÑ‚ÑŒ, Ñ‚ÐµÑÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ
"""
import ast
import os
import re
import json
from pathlib import Path
from typing import Dict, List, Any, Tuple
from collections import defaultdict, Counter


class CodeQualityAnalyzer:
    """ÐÐ½Ð°Ð»Ð¸Ð·Ð°Ñ‚Ð¾Ñ€ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð° ÐºÐ¾Ð´Ð°"""

    def __init__(self, project_root: str = "."):
        self.project_root = Path(project_root)
        self.python_files = []
        self.analysis_results = {}

    def discover_python_files(self) -> List[Path]:
        """ÐÐ°Ð¹Ñ‚Ð¸ Ð²ÑÐµ Python Ñ„Ð°Ð¹Ð»Ñ‹ Ð² Ð¿Ñ€Ð¾ÐµÐºÑ‚Ðµ"""
        python_files = []

        # ÐžÑÐ½Ð¾Ð²Ð½Ñ‹Ðµ Ð´Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ð¸Ð¸ Ð´Ð»Ñ Ð°Ð½Ð°Ð»Ð¸Ð·Ð°
        target_dirs = ["agent_runtime", "agent_system", "."]  # ÐºÐ¾Ñ€Ð½ÐµÐ²Ñ‹Ðµ Ñ„Ð°Ð¹Ð»Ñ‹

        for target_dir in target_dirs:
            dir_path = self.project_root / target_dir
            if dir_path.exists():
                for py_file in dir_path.rglob("*.py"):
                    # Ð˜ÑÐºÐ»ÑŽÑ‡Ð°ÐµÐ¼ Ð½ÐµÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ Ñ„Ð°Ð¹Ð»Ñ‹
                    if not any(
                        exclude in str(py_file)
                        for exclude in ["__pycache__", ".venv", "venv", ".git", "test_", "lora_qwen", "codesearchnet"]
                    ):
                        python_files.append(py_file)

        self.python_files = python_files
        return python_files

    def analyze_file_metrics(self, file_path: Path) -> Dict[str, Any]:
        """ÐÐ½Ð°Ð»Ð¸Ð· Ð¼ÐµÑ‚Ñ€Ð¸Ðº Ñ„Ð°Ð¹Ð»Ð°"""
        try:
            content = file_path.read_text(encoding="utf-8")
            lines = content.splitlines()

            # Ð‘Ð°Ð·Ð¾Ð²Ñ‹Ðµ Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ¸
            metrics = {
                "file_path": str(file_path.relative_to(self.project_root)),
                "total_lines": len(lines),
                "code_lines": len([line for line in lines if line.strip() and not line.strip().startswith("#")]),
                "comment_lines": len([line for line in lines if line.strip().startswith("#")]),
                "blank_lines": len([line for line in lines if not line.strip()]),
                "file_size_kb": round(len(content) / 1024, 2),
            }

            # ÐÐ½Ð°Ð»Ð¸Ð· AST
            try:
                tree = ast.parse(content)
                ast_metrics = self._analyze_ast(tree)
                metrics.update(ast_metrics)
            except SyntaxError as e:
                metrics["syntax_error"] = str(e)
                metrics["ast_analysis"] = False
            else:
                metrics["ast_analysis"] = True

            # ÐÐ½Ð°Ð»Ð¸Ð· ÑÑ‚Ð¸Ð»Ñ
            style_issues = self._analyze_style(content, lines)
            metrics["style_issues"] = style_issues

            # ÐÐ½Ð°Ð»Ð¸Ð· ÑÐ»Ð¾Ð¶Ð½Ð¾ÑÑ‚Ð¸
            complexity = self._analyze_complexity(content, lines)
            metrics["complexity"] = complexity

            # ÐÐ½Ð°Ð»Ð¸Ð· Ð±ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ð¾ÑÑ‚Ð¸
            security_issues = self._analyze_security(content)
            metrics["security_issues"] = security_issues

            return metrics

        except Exception as e:
            return {
                "file_path": str(file_path.relative_to(self.project_root)),
                "error": str(e),
                "analysis_failed": True,
            }

    def _analyze_ast(self, tree: ast.AST) -> Dict[str, Any]:
        """ÐÐ½Ð°Ð»Ð¸Ð· AST Ð´ÐµÑ€ÐµÐ²Ð°"""
        classes = []
        functions = []
        imports = []

        for node in ast.walk(tree):
            if isinstance(node, ast.ClassDef):
                classes.append(
                    {
                        "name": node.name,
                        "line": node.lineno,
                        "methods": len([n for n in node.body if isinstance(n, ast.FunctionDef)]),
                        "docstring": ast.get_docstring(node) is not None,
                    }
                )
            elif isinstance(node, ast.FunctionDef):
                # Ð¢Ð¾Ð»ÑŒÐºÐ¾ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¸ Ð²ÐµÑ€Ñ…Ð½ÐµÐ³Ð¾ ÑƒÑ€Ð¾Ð²Ð½Ñ (Ð½Ðµ Ð¼ÐµÑ‚Ð¾Ð´Ñ‹)
                if not any(
                    isinstance(parent, ast.ClassDef)
                    for parent in ast.walk(tree)
                    if hasattr(parent, "body") and node in getattr(parent, "body", [])
                ):
                    functions.append(
                        {
                            "name": node.name,
                            "line": node.lineno,
                            "args_count": len(node.args.args),
                            "docstring": ast.get_docstring(node) is not None,
                            "is_async": isinstance(node, ast.AsyncFunctionDef),
                        }
                    )
            elif isinstance(node, (ast.Import, ast.ImportFrom)):
                if isinstance(node, ast.Import):
                    for alias in node.names:
                        imports.append(alias.name)
                else:
                    imports.append(node.module or "relative")

        return {
            "classes": classes,
            "functions": functions,
            "imports": imports,
            "class_count": len(classes),
            "function_count": len(functions),
            "import_count": len(imports),
        }

    def _analyze_style(self, content: str, lines: List[str]) -> List[Dict[str, Any]]:
        """ÐÐ½Ð°Ð»Ð¸Ð· ÑÑ‚Ð¸Ð»Ñ ÐºÐ¾Ð´Ð°"""
        issues = []

        for i, line in enumerate(lines, 1):
            # Ð”Ð»Ð¸Ð½Ð° ÑÑ‚Ñ€Ð¾ÐºÐ¸
            if len(line) > 120:
                issues.append(
                    {
                        "type": "line_length",
                        "line": i,
                        "message": f"Line too long ({len(line)} > 120 chars)",
                        "severity": "warning",
                    }
                )

            # Trailing whitespace
            if line.endswith(" ") or line.endswith("\t"):
                issues.append(
                    {"type": "trailing_whitespace", "line": i, "message": "Trailing whitespace", "severity": "info"}
                )

            # ÐœÐ½Ð¾Ð¶ÐµÑÑ‚Ð²ÐµÐ½Ð½Ñ‹Ðµ Ð¸Ð¼Ð¿Ð¾Ñ€Ñ‚Ñ‹
            if line.strip().startswith("import ") and "," in line:
                issues.append(
                    {
                        "type": "multiple_imports",
                        "line": i,
                        "message": "Multiple imports on one line",
                        "severity": "warning",
                    }
                )

            # Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ðµ print() (Ð¼Ð¾Ð¶ÐµÑ‚ Ð±Ñ‹Ñ‚ÑŒ debug)
            if "print(" in line and not line.strip().startswith("#"):
                issues.append(
                    {
                        "type": "print_statement",
                        "line": i,
                        "message": "Print statement (consider logging)",
                        "severity": "info",
                    }
                )

        # ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° docstrings
        if '"""' not in content and "'''" not in content:
            issues.append(
                {"type": "no_docstrings", "line": 1, "message": "No docstrings found in file", "severity": "warning"}
            )

        return issues

    def _analyze_complexity(self, content: str, lines: List[str]) -> Dict[str, Any]:
        """ÐÐ½Ð°Ð»Ð¸Ð· ÑÐ»Ð¾Ð¶Ð½Ð¾ÑÑ‚Ð¸ ÐºÐ¾Ð´Ð°"""

        # Ð¦Ð¸ÐºÐ»Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ°Ñ ÑÐ»Ð¾Ð¶Ð½Ð¾ÑÑ‚ÑŒ (Ð¿Ñ€Ð¸Ð±Ð»Ð¸Ð·Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð°Ñ)
        complexity_keywords = ["if", "elif", "else", "for", "while", "try", "except", "with"]
        complexity_score = 0

        for line in lines:
            stripped = line.strip()
            for keyword in complexity_keywords:
                if stripped.startswith(keyword + " ") or stripped.startswith(keyword + ":"):
                    complexity_score += 1

        # Ð£Ñ€Ð¾Ð²ÐµÐ½ÑŒ Ð²Ð»Ð¾Ð¶ÐµÐ½Ð½Ð¾ÑÑ‚Ð¸
        max_nesting = 0
        current_nesting = 0

        for line in lines:
            if line.strip():
                indent = len(line) - len(line.lstrip())
                current_nesting = indent // 4
                max_nesting = max(max_nesting, current_nesting)

        # Ð”Ð»Ð¸Ð½Ð½Ñ‹Ðµ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¸ (ÑÐ²Ñ€Ð¸ÑÑ‚Ð¸ÐºÐ°)
        long_functions = []
        in_function = False
        function_start = 0
        function_name = ""

        for i, line in enumerate(lines, 1):
            stripped = line.strip()
            if stripped.startswith("def ") or stripped.startswith("async def "):
                if in_function and i - function_start > 50:
                    long_functions.append(
                        {"name": function_name, "start_line": function_start, "length": i - function_start}
                    )
                in_function = True
                function_start = i
                function_name = stripped.split("(")[0].replace("def ", "").replace("async ", "")
            elif stripped.startswith("class "):
                in_function = False

        return {
            "cyclomatic_complexity": complexity_score,
            "max_nesting_level": max_nesting,
            "long_functions": long_functions,
            "complexity_score": complexity_score + max_nesting * 2,
        }

    def _analyze_security(self, content: str) -> List[Dict[str, Any]]:
        """ÐÐ½Ð°Ð»Ð¸Ð· Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼ Ð±ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ð¾ÑÑ‚Ð¸"""
        issues = []

        security_patterns = [
            (r'password\s*=\s*["\'][^"\']+["\']', "Hardcoded password", "high"),
            (r'api_key\s*=\s*["\'][^"\']+["\']', "Hardcoded API key", "high"),
            (r'secret\s*=\s*["\'][^"\']+["\']', "Hardcoded secret", "high"),
            (r"eval\s*\(", "Use of eval() function", "high"),
            (r"exec\s*\(", "Use of exec() function", "high"),
            (r"subprocess\.call\([^)]*shell=True", "Shell injection risk", "medium"),
            (r"os\.system\(", "OS command injection risk", "medium"),
            (r"pickle\.loads?\(", "Unsafe pickle usage", "medium"),
            (r"input\s*\(", "Use of input() function", "low"),
            (r'open\s*\([^)]*["\'][wax]', "File write operations", "info"),
        ]

        lines = content.splitlines()
        for pattern, description, severity in security_patterns:
            for match in re.finditer(pattern, content, re.IGNORECASE):
                line_num = content[: match.start()].count("\n") + 1
                issues.append(
                    {
                        "type": "security",
                        "line": line_num,
                        "message": description,
                        "severity": severity,
                        "pattern": pattern,
                    }
                )

        return issues

    def analyze_project_structure(self) -> Dict[str, Any]:
        """ÐÐ½Ð°Ð»Ð¸Ð· ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ñ‹ Ð¿Ñ€Ð¾ÐµÐºÑ‚Ð°"""

        structure = {
            "total_files": len(self.python_files),
            "directories": defaultdict(int),
            "file_types": defaultdict(int),
            "module_analysis": {},
        }

        for file_path in self.python_files:
            # ÐŸÐ¾Ð´ÑÑ‡ÐµÑ‚ Ð¿Ð¾ Ð´Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ð¸ÑÐ¼
            parent_dir = file_path.parent.name
            structure["directories"][parent_dir] += 1

            # ÐÐ½Ð°Ð»Ð¸Ð· Ð¼Ð¾Ð´ÑƒÐ»ÐµÐ¹
            rel_path = file_path.relative_to(self.project_root)
            if len(rel_path.parts) > 1:
                module = rel_path.parts[0]
                if module not in structure["module_analysis"]:
                    structure["module_analysis"][module] = {"files": 0, "has_init": False, "submodules": set()}
                structure["module_analysis"][module]["files"] += 1

                # ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° __init__.py
                init_file = file_path.parent / "__init__.py"
                if init_file.exists():
                    structure["module_analysis"][module]["has_init"] = True

                # ÐŸÐ¾Ð´Ð¼Ð¾Ð´ÑƒÐ»Ð¸
                if len(rel_path.parts) > 2:
                    structure["module_analysis"][module]["submodules"].add(rel_path.parts[1])

        # ÐšÐ¾Ð½Ð²ÐµÑ€Ñ‚Ð¸Ñ€ÑƒÐµÐ¼ sets Ð² lists Ð´Ð»Ñ JSON
        for module_info in structure["module_analysis"].values():
            module_info["submodules"] = list(module_info["submodules"])

        return structure

    def analyze_dependencies(self) -> Dict[str, Any]:
        """ÐÐ½Ð°Ð»Ð¸Ð· Ð·Ð°Ð²Ð¸ÑÐ¸Ð¼Ð¾ÑÑ‚ÐµÐ¹"""

        all_imports = []
        import_graph = defaultdict(set)

        for file_path in self.python_files:
            try:
                content = file_path.read_text(encoding="utf-8")
                tree = ast.parse(content)

                file_imports = []
                for node in ast.walk(tree):
                    if isinstance(node, ast.Import):
                        for alias in node.names:
                            file_imports.append(alias.name)
                            all_imports.append(alias.name)
                    elif isinstance(node, ast.ImportFrom):
                        module = node.module or "relative"
                        file_imports.append(module)
                        all_imports.append(module)

                # Ð“Ñ€Ð°Ñ„ Ð·Ð°Ð²Ð¸ÑÐ¸Ð¼Ð¾ÑÑ‚ÐµÐ¹
                rel_path = str(file_path.relative_to(self.project_root))
                import_graph[rel_path] = set(file_imports)

            except Exception:
                continue

        # ÐÐ½Ð°Ð»Ð¸Ð· Ð¿Ð¾Ð¿ÑƒÐ»ÑÑ€Ð½Ñ‹Ñ… Ð¸Ð¼Ð¿Ð¾Ñ€Ñ‚Ð¾Ð²
        import_counter = Counter(all_imports)

        # Ð’Ð½ÐµÑˆÐ½Ð¸Ðµ vs Ð²Ð½ÑƒÑ‚Ñ€ÐµÐ½Ð½Ð¸Ðµ Ð·Ð°Ð²Ð¸ÑÐ¸Ð¼Ð¾ÑÑ‚Ð¸
        external_deps = set()
        internal_deps = set()

        for imp in all_imports:
            if imp.startswith(("agent_runtime", "agent_system")):
                internal_deps.add(imp)
            elif not imp.startswith(".") and imp != "relative":
                external_deps.add(imp)

        return {
            "total_imports": len(all_imports),
            "unique_imports": len(set(all_imports)),
            "external_dependencies": list(external_deps),
            "internal_dependencies": list(internal_deps),
            "most_used_imports": import_counter.most_common(10),
            "import_graph_size": len(import_graph),
        }

    def calculate_quality_scores(self, file_metrics: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Ð Ð°ÑÑ‡ÐµÑ‚ Ð¾Ñ†ÐµÐ½Ð¾Ðº ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð°"""

        if not file_metrics:
            return {"error": "No files analyzed"}

        # Ð¤Ð¸Ð»ÑŒÑ‚Ñ€ÑƒÐµÐ¼ Ñ„Ð°Ð¹Ð»Ñ‹ Ñ Ð¾ÑˆÐ¸Ð±ÐºÐ°Ð¼Ð¸
        valid_files = [f for f in file_metrics if not f.get("analysis_failed", False)]

        if not valid_files:
            return {"error": "No valid files analyzed"}

        # ÐœÐµÑ‚Ñ€Ð¸ÐºÐ¸ Ð¿Ð¾ Ð²ÑÐµÐ¼Ñƒ Ð¿Ñ€Ð¾ÐµÐºÑ‚Ñƒ
        total_lines = sum(f["total_lines"] for f in valid_files)
        total_code_lines = sum(f["code_lines"] for f in valid_files)
        total_comment_lines = sum(f["comment_lines"] for f in valid_files)

        # ÐžÑ†ÐµÐ½ÐºÐ¸ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð° (0-10)
        scores = {}

        # 1. Ð”Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð¾ÑÑ‚ÑŒ (docstrings + ÐºÐ¾Ð¼Ð¼ÐµÐ½Ñ‚Ð°Ñ€Ð¸Ð¸)
        files_with_docstrings = sum(
            1
            for f in valid_files
            if f.get("ast_analysis")
            and any(cls.get("docstring", False) for cls in f.get("classes", []))
            or any(func.get("docstring", False) for func in f.get("functions", []))
        )

        comment_ratio = total_comment_lines / total_code_lines if total_code_lines > 0 else 0
        documentation_score = min(10, (files_with_docstrings / len(valid_files)) * 5 + comment_ratio * 50)
        scores["documentation"] = round(documentation_score, 1)

        # 2. Ð¡Ð»Ð¾Ð¶Ð½Ð¾ÑÑ‚ÑŒ ÐºÐ¾Ð´Ð°
        avg_complexity = sum(f.get("complexity", {}).get("complexity_score", 0) for f in valid_files) / len(valid_files)
        max_nesting = max(f.get("complexity", {}).get("max_nesting_level", 0) for f in valid_files)
        complexity_score = max(0, 10 - (avg_complexity / 10) - (max_nesting / 2))
        scores["complexity"] = round(complexity_score, 1)

        # 3. Ð¡Ñ‚Ð¸Ð»ÑŒ ÐºÐ¾Ð´Ð°
        total_style_issues = sum(len(f.get("style_issues", [])) for f in valid_files)
        style_score = max(0, 10 - (total_style_issues / len(valid_files)))
        scores["style"] = round(style_score, 1)

        # 4. Ð‘ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ð¾ÑÑ‚ÑŒ
        high_security_issues = sum(
            len([issue for issue in f.get("security_issues", []) if issue.get("severity") == "high"])
            for f in valid_files
        )
        medium_security_issues = sum(
            len([issue for issue in f.get("security_issues", []) if issue.get("severity") == "medium"])
            for f in valid_files
        )

        security_score = max(0, 10 - high_security_issues * 2 - medium_security_issues * 0.5)
        scores["security"] = round(security_score, 1)

        # 5. ÐÑ€Ñ…Ð¸Ñ‚ÐµÐºÑ‚ÑƒÑ€Ð° (Ð¼Ð¾Ð´ÑƒÐ»ÑŒÐ½Ð¾ÑÑ‚ÑŒ)
        total_classes = sum(f.get("class_count", 0) for f in valid_files)
        total_functions = sum(f.get("function_count", 0) for f in valid_files)
        avg_file_size = sum(f["total_lines"] for f in valid_files) / len(valid_files)

        # Ð¥Ð¾Ñ€Ð¾ÑˆÐ°Ñ Ð°Ñ€Ñ…Ð¸Ñ‚ÐµÐºÑ‚ÑƒÑ€Ð° = ÑƒÐ¼ÐµÑ€ÐµÐ½Ð½Ñ‹Ð¹ Ñ€Ð°Ð·Ð¼ÐµÑ€ Ñ„Ð°Ð¹Ð»Ð¾Ð², Ñ…Ð¾Ñ€Ð¾ÑˆÐµÐµ ÑÐ¾Ð¾Ñ‚Ð½Ð¾ÑˆÐµÐ½Ð¸Ðµ ÐºÐ»Ð°ÑÑÐ¾Ð²/Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¹
        architecture_score = min(
            10,
            (10 - max(0, (avg_file_size - 200) / 50)) * 0.4
            + min(10, (total_classes + total_functions) / len(valid_files))  # Ñ€Ð°Ð·Ð¼ÐµÑ€ Ñ„Ð°Ð¹Ð»Ð¾Ð²
            * 0.6,  # ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð¾ÑÑ‚ÑŒ
        )
        scores["architecture"] = round(architecture_score, 1)

        # 6. ÐžÐ±Ñ‰Ð°Ñ Ð¾Ñ†ÐµÐ½ÐºÐ°
        overall_score = sum(scores.values()) / len(scores)
        scores["overall"] = round(overall_score, 1)

        return {
            "scores": scores,
            "metrics": {
                "total_files": len(valid_files),
                "total_lines": total_lines,
                "total_code_lines": total_code_lines,
                "total_comment_lines": total_comment_lines,
                "comment_ratio": round(comment_ratio * 100, 1),
                "avg_file_size": round(avg_file_size, 1),
                "total_classes": total_classes,
                "total_functions": total_functions,
                "total_style_issues": total_style_issues,
                "high_security_issues": high_security_issues,
                "medium_security_issues": medium_security_issues,
            },
        }

    def generate_report(self) -> Dict[str, Any]:
        """Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ Ð¿Ð¾Ð»Ð½Ð¾Ð³Ð¾ Ð¾Ñ‚Ñ‡ÐµÑ‚Ð°"""

        print("ðŸ” ÐÐ½Ð°Ð»Ð¸Ð· ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð° ÐºÐ¾Ð´Ð° Ð¼ÑƒÐ»ÑŒÑ‚Ð¸Ð°Ð³ÐµÐ½Ñ‚Ð½Ð¾Ð¹ ÑÐ¸ÑÑ‚ÐµÐ¼Ñ‹")
        print("=" * 60)

        # ÐŸÐ¾Ð¸ÑÐº Ñ„Ð°Ð¹Ð»Ð¾Ð²
        print("ðŸ“ ÐŸÐ¾Ð¸ÑÐº Python Ñ„Ð°Ð¹Ð»Ð¾Ð²...")
        python_files = self.discover_python_files()
        print(f"   ÐÐ°Ð¹Ð´ÐµÐ½Ð¾ Ñ„Ð°Ð¹Ð»Ð¾Ð²: {len(python_files)}")

        # ÐÐ½Ð°Ð»Ð¸Ð· ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ñ‹ Ð¿Ñ€Ð¾ÐµÐºÑ‚Ð°
        print("ðŸ—ï¸ ÐÐ½Ð°Ð»Ð¸Ð· ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ñ‹ Ð¿Ñ€Ð¾ÐµÐºÑ‚Ð°...")
        structure = self.analyze_project_structure()

        # ÐÐ½Ð°Ð»Ð¸Ð· Ð·Ð°Ð²Ð¸ÑÐ¸Ð¼Ð¾ÑÑ‚ÐµÐ¹
        print("ðŸ“¦ ÐÐ½Ð°Ð»Ð¸Ð· Ð·Ð°Ð²Ð¸ÑÐ¸Ð¼Ð¾ÑÑ‚ÐµÐ¹...")
        dependencies = self.analyze_dependencies()

        # ÐÐ½Ð°Ð»Ð¸Ð· ÐºÐ°Ð¶Ð´Ð¾Ð³Ð¾ Ñ„Ð°Ð¹Ð»Ð°
        print("ðŸ“Š ÐÐ½Ð°Ð»Ð¸Ð· Ñ„Ð°Ð¹Ð»Ð¾Ð²...")
        file_metrics = []
        for i, file_path in enumerate(python_files, 1):
            print(f"   ÐÐ½Ð°Ð»Ð¸Ð· {i}/{len(python_files)}: {file_path.name}")
            metrics = self.analyze_file_metrics(file_path)
            file_metrics.append(metrics)

        # Ð Ð°ÑÑ‡ÐµÑ‚ Ð¾Ñ†ÐµÐ½Ð¾Ðº ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð°
        print("ðŸŽ¯ Ð Ð°ÑÑ‡ÐµÑ‚ Ð¾Ñ†ÐµÐ½Ð¾Ðº ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð°...")
        quality_scores = self.calculate_quality_scores(file_metrics)

        return {
            "analysis_timestamp": "2026-01-06",
            "project_structure": structure,
            "dependencies": dependencies,
            "file_metrics": file_metrics,
            "quality_scores": quality_scores,
            "summary": {
                "total_python_files": len(python_files),
                "analysis_successful": len([f for f in file_metrics if not f.get("analysis_failed", False)]),
                "analysis_failed": len([f for f in file_metrics if f.get("analysis_failed", False)]),
            },
        }


def main():
    """ÐžÑÐ½Ð¾Ð²Ð½Ð°Ñ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ñ Ð°Ð½Ð°Ð»Ð¸Ð·Ð°"""

    analyzer = CodeQualityAnalyzer()
    report = analyzer.generate_report()

    print("\n" + "=" * 60)
    print("ðŸ“‹ Ð Ð•Ð—Ð£Ð›Ð¬Ð¢ÐÐ¢Ð« ÐÐÐÐ›Ð˜Ð—Ð ÐšÐÐ§Ð•Ð¡Ð¢Ð’Ð ÐšÐžÐ”Ð")
    print("=" * 60)

    # Ð¡Ñ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ð° Ð¿Ñ€Ð¾ÐµÐºÑ‚Ð°
    structure = report["project_structure"]
    print(f"\nðŸ—ï¸ Ð¡Ñ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ð° Ð¿Ñ€Ð¾ÐµÐºÑ‚Ð°:")
    print(f"   Ð’ÑÐµÐ³Ð¾ Python Ñ„Ð°Ð¹Ð»Ð¾Ð²: {structure['total_files']}")
    print(f"   ÐœÐ¾Ð´ÑƒÐ»Ð¸: {list(structure['module_analysis'].keys())}")

    # Ð—Ð°Ð²Ð¸ÑÐ¸Ð¼Ð¾ÑÑ‚Ð¸
    deps = report["dependencies"]
    print(f"\nðŸ“¦ Ð—Ð°Ð²Ð¸ÑÐ¸Ð¼Ð¾ÑÑ‚Ð¸:")
    print(f"   Ð’ÑÐµÐ³Ð¾ Ð¸Ð¼Ð¿Ð¾Ñ€Ñ‚Ð¾Ð²: {deps['total_imports']}")
    print(f"   Ð’Ð½ÐµÑˆÐ½Ð¸Ðµ Ð·Ð°Ð²Ð¸ÑÐ¸Ð¼Ð¾ÑÑ‚Ð¸: {len(deps['external_dependencies'])}")
    print(f"   Ð’Ð½ÑƒÑ‚Ñ€ÐµÐ½Ð½Ð¸Ðµ Ð·Ð°Ð²Ð¸ÑÐ¸Ð¼Ð¾ÑÑ‚Ð¸: {len(deps['internal_dependencies'])}")

    # ÐžÑ†ÐµÐ½ÐºÐ¸ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð°
    if "scores" in report["quality_scores"]:
        scores = report["quality_scores"]["scores"]
        metrics = report["quality_scores"]["metrics"]

        print(f"\nðŸŽ¯ ÐžÑ†ÐµÐ½ÐºÐ¸ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð° (0-10):")
        print(f"   ðŸ“š Ð”Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð¾ÑÑ‚ÑŒ: {scores['documentation']}/10")
        print(f"   ðŸ§® Ð¡Ð»Ð¾Ð¶Ð½Ð¾ÑÑ‚ÑŒ ÐºÐ¾Ð´Ð°: {scores['complexity']}/10")
        print(f"   ðŸŽ¨ Ð¡Ñ‚Ð¸Ð»ÑŒ ÐºÐ¾Ð´Ð°: {scores['style']}/10")
        print(f"   ðŸ”’ Ð‘ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ð¾ÑÑ‚ÑŒ: {scores['security']}/10")
        print(f"   ðŸ—ï¸ ÐÑ€Ñ…Ð¸Ñ‚ÐµÐºÑ‚ÑƒÑ€Ð°: {scores['architecture']}/10")
        print(f"   ðŸ† ÐžÐ±Ñ‰Ð°Ñ Ð¾Ñ†ÐµÐ½ÐºÐ°: {scores['overall']}/10")

        print(f"\nðŸ“Š ÐœÐµÑ‚Ñ€Ð¸ÐºÐ¸:")
        print(f"   Ð¡Ñ‚Ñ€Ð¾Ðº ÐºÐ¾Ð´Ð°: {metrics['total_code_lines']:,}")
        print(f"   ÐšÐ¾Ð¼Ð¼ÐµÐ½Ñ‚Ð°Ñ€Ð¸ÐµÐ²: {metrics['comment_ratio']}%")
        print(f"   ÐšÐ»Ð°ÑÑÐ¾Ð²: {metrics['total_classes']}")
        print(f"   Ð¤ÑƒÐ½ÐºÑ†Ð¸Ð¹: {metrics['total_functions']}")
        print(f"   ÐŸÑ€Ð¾Ð±Ð»ÐµÐ¼Ñ‹ ÑÑ‚Ð¸Ð»Ñ: {metrics['total_style_issues']}")
        print(f"   ÐšÑ€Ð¸Ñ‚Ð¸Ñ‡Ð½Ñ‹Ðµ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ñ‹ Ð±ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ð¾ÑÑ‚Ð¸: {metrics['high_security_issues']}")

        # ÐžÐ±Ñ‰Ð°Ñ Ð¾Ñ†ÐµÐ½ÐºÐ°
        overall = scores["overall"]
        if overall >= 8.0:
            quality_level = "ÐžÐ¢Ð›Ð˜Ð§ÐÐž"
            emoji = "ðŸ†"
        elif overall >= 7.0:
            quality_level = "Ð¥ÐžÐ ÐžÐ¨Ðž"
            emoji = "âœ…"
        elif overall >= 6.0:
            quality_level = "Ð£Ð”ÐžÐ’Ð›Ð•Ð¢Ð’ÐžÐ Ð˜Ð¢Ð•Ð›Ð¬ÐÐž"
            emoji = "âš ï¸"
        else:
            quality_level = "Ð¢Ð Ð•Ð‘Ð£Ð•Ð¢ Ð£Ð›Ð£Ð§Ð¨Ð•ÐÐ˜Ð¯"
            emoji = "âŒ"

        print(f"\n{emoji} ÐžÐ‘Ð©ÐÐ¯ ÐžÐ¦Ð•ÐÐšÐ ÐšÐÐ§Ð•Ð¡Ð¢Ð’Ð ÐšÐžÐ”Ð: {quality_level} ({overall}/10)")

    # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ Ð¾Ñ‚Ñ‡ÐµÑ‚Ð°
    with open("code_quality_report.json", "w", encoding="utf-8") as f:
        json.dump(report, f, indent=2, ensure_ascii=False, default=str)

    print(f"\nðŸ’¾ Ð”ÐµÑ‚Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ð¾Ñ‚Ñ‡ÐµÑ‚ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½ Ð²: code_quality_report.json")

    return report["quality_scores"]["scores"]["overall"] >= 6.0 if "scores" in report["quality_scores"] else False


if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)
