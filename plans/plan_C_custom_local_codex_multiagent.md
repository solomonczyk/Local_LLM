# План C — Свой локальный “Codex” + инструменты + мультиагентный консилиум (рекомендовано)

Цель: создать свой агентный runtime:
- OpenAI-compatible LLM сервер (у тебя есть)
- Tool server (fs/git/shell) с безопасностью
- Оркестратор мультиагентов (director + специалисты)
- Интеграция с Continue (как UI) или отдельный веб/CLI

---

## Архитектура (в 4 компонента)
1) LLM Server: `serve_lora.py` (chat completions)
2) Tool Server: отдельный FastAPI сервис `/tools/*`
3) Orchestrator: Python модуль, управляющий агентами и вызовами tools
4) UI: Continue (VS Code) или CLI/веб

---

## Этап 0. Контракты и безопасная модель доступа
1. Определить рабочую директорию проекта (workspace root).
2. Ввести “политику выполнения”:
   - read/write только внутри workspace
   - shell только allowlist
   - git только безопасные команды
3. Вести audit-log:
   - кто попросил
   - что выполнено
   - результат

Результат: управляемая автономность без катастроф.

---

## Этап 1. Tool Server (FastAPI)
Сделать эндпоинты:
- POST `/tools/read_file` {path}
- POST `/tools/write_file` {path, content, mode: overwrite|patch}
- POST `/tools/list_dir` {path}
- POST `/tools/search` {query, globs}
- POST `/tools/git` {cmd: status|diff|commit|checkout|...}
- POST `/tools/shell` {command} (allowlist)

Требования:
- проверка путей (не выходить из workspace)
- лимиты размера файлов
- таймаут на команды
- запрет сетевых команд по умолчанию

Результат: “руки” агента.

---

## Этап 2. Orchestrator: единый цикл работы агента
Схема:
1) Получить задачу
2) Сформировать план
3) Запросить консилиум
4) Выполнить изменения (patch)
5) Запустить тесты/линтеры
6) Если ок → финальный отчёт + предложить коммит

Результат: автономная разработка с контролем.

---

## Этап 3. Мультиагентный консилиум
Роли:
- Director (решение + последовательность)
- Architect (структура, масштабирование)
- Security (секреты, права, риски)
- QA (тесты, edge cases)
- Dev (реализация)
- Reviewer (стиль, рефакторинг)

Протокол:
- каждый агент получает одинаковый контекст + ограничения tools
- выдаёт:
  - “Решение”
  - “Риски”
  - “Проверки”
  - “Скоринг уверенности 0–10”
- Director агрегирует:
  - выбирает план
  - формирует “Execution checklist”

Результат: “коллеги” в голове, но системно.

---

## Этап 4. Интеграция с Continue (VS Code)
Варианты:
A) Continue → (LLM server) + Orchestrator отдельно (по кнопке/CLI)
B) Continue → Orchestrator как OpenAI-compatible endpoint (он уже сам вызывает LLM + tools)

Рекомендуется B:
- Continue думает, что общается с “моделью”
- а на самом деле это “агентный слой”, который:
  - зовёт LLM
  - зовёт tools
  - возвращает итог

Результат: вайбкодинг прямо в VS Code, но с автономностью.

---

## Этап 5. Режимы доступа к ресурсам (уровни)
Level 0: read-only (чтение файлов, поиск)
Level 1: safe write (правки только через patch)
Level 2: run tests (pytest/npm test)
Level 3: git commit (только после green checks)
Level 4: shell extended (только вручную включается)

Результат: безопасная “эволюция” автономности.

---

## Этап 6. Streaming и UX
1. Streaming ответов (чтобы “печатало”)
2. Прогресс-ивенты:
   - “анализ”
   - “консилиум”
   - “патч”
   - “тесты”
3. Отчёт:
   - что изменено
   - какие команды выполнены
   - результаты тестов

Результат: ощущение “живого Codex”.

---

## Этап 7. Надёжность и контроль качества
- retries на вызовы модели
- лимиты токенов/времени
- защита от бесконечных циклов
- snapshot файлов перед изменениями
- auto-rollback при провале тестов

Результат: агент не ломает проект.

---

## Критерий успеха (Definition of Done)
- Задача → консилиум → патч → тесты → отчёт → (опционально) коммит.
- Всё локально, управляемо и воспроизводимо.

---

## Почему этот вариант лучший для тебя
- Ты уже умеешь Docker/оркестрацию и хочешь мультиагентность.
- Ты контролируешь безопасность и не зависишь от чужих решений.
- Можно наращивать автономность без “всё или ничего”.
